import streamlit as st
from google import genai
from google.genai import types 


st.set_page_config(page_title="ü§ñ My Gemini Chatbot", layout="wide")
st.title("ü§ñ My Gemini Chatbot")

@st.cache_resource
def get_gemini_client():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ client ‡∏Ç‡∏≠‡∏á Gemini API"""
    try:
        return genai.Client(api_key=st.secrets["GEMINI_API_KEY"])
    except KeyError:
        st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö **GEMINI_API_KEY** ‡πÉ‡∏ô `st.secrets` ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
        st.stop()

client = get_gemini_client()

if "messages" not in st.session_state:
    system_instruction = "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ AI Chatbot ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏†‡∏≤‡∏û"
    st.session_state["messages"] = [
        {"role": "system", "content": system_instruction}
    ]

def prepare_gemini_messages(messages):
    gemini_messages = []
    
    for msg in messages:
        if msg["role"] == "system":
            continue
            
        role = 'model' if msg["role"] == 'assistant' else msg["role"]
        gemini_messages.append(
            types.Content(
                role=role, 
                parts=[types.Part.from_text(text=msg["content"])] 
            )
        )
    return gemini_messages

for msg in st.session_state["messages"][1:]:
    st.chat_message(msg["role"]).write(msg["content"])


user_input = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")

if user_input:
    st.session_state["messages"].append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    system_instruction = st.session_state["messages"][0]["content"]
    gemini_messages = prepare_gemini_messages(st.session_state["messages"])
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        try:
            response_stream = client.models.generate_content_stream(
                model="gemini-2.5-flash",
                contents=gemini_messages,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                )
            )
            
            for chunk in response_stream:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "‚ñå") 
            
            message_placeholder.markdown(full_response)
            ai_reply = full_response
            
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini API: {e}")
            ai_reply = f"Error: {e}"

    st.session_state["messages"].append({"role": "assistant", "content": ai_reply})